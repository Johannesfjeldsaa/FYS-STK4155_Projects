{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cad4281a763db6",
   "metadata": {},
   "source": [
    "# Neural Network with PyTorch framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efef9246badaac9",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-10T12:54:25.398031500Z",
     "start_time": "2023-11-10T12:54:19.448421400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from Neural_Network_with_PyTorch import Neural_Network_PyTorch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d705eb5bf84d3",
   "metadata": {},
   "source": [
    "## Create Neural network and perform passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f52acacf316be3",
   "metadata": {},
   "source": [
    "1. Set up the design matrix with the inputs as discussed above and a vector containing the output, the so-called targets. Note that the design matrix is the same for all gates. You need just to define different outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7839efb5e03f7e",
   "metadata": {},
   "source": [
    "We start by defining a helper function that allows us to convert a numpy array to a PyTorch tensor, with or without the gradient flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6ad790cc4362ad",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-10T12:54:25.406513500Z",
     "start_time": "2023-11-10T12:54:25.402386200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create design matrix\n",
    "X = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).requires_grad_(True)\n",
    "\n",
    "# The XOR gate\n",
    "target_XOR = torch.Tensor([ 0, 1 ,1, 0]).view(-1, 1)\n",
    "# The OR gate\n",
    "target_OR = torch.Tensor([ 0, 1 ,1, 1]).view(-1, 1)\n",
    "# The AND gate\n",
    "target_AND = torch.Tensor([ 0, 0 ,0, 1]).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28088293c725bdc",
   "metadata": {},
   "source": [
    "2. Construct a neural network with only one hidden layer and two hidden nodes using the Sigmoid function as activation function.\n",
    "3. Set up the output layer with only one output node and use again the Sigmoid function as activation function for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca689bc2603bc9",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-10T12:54:25.432797Z",
     "start_time": "2023-11-10T12:54:25.408609500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid()\n",
      "Model parameters\n",
      "[Parameter containing:\n",
      "tensor([[ 0.6733,  0.4288],\n",
      "        [ 0.5845, -0.1015]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4355,  0.4374], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2478,  0.1221]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5650], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# Network design\n",
    "n_inputs = X.shape[1]\n",
    "n_hidden_layers = 1\n",
    "n_hidden_nodes = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# Create network\n",
    "ffnn = Neural_Network_PyTorch(n_inputs, \n",
    "                              n_hidden_layers, n_hidden_nodes, \n",
    "                              n_outputs, \n",
    "                              activation_function_hidden_layers='sigmoid',\n",
    "                              activation_function_output_layer='sigmoid')\n",
    "\n",
    "print('Model parameters')\n",
    "print(list(ffnn.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b3d3307d5ccd",
   "metadata": {},
   "source": [
    "5. Set up the cost function (cross entropy for classification of binary cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18afac54d5b5fc6",
   "metadata": {},
   "source": [
    "The evaluation criterion in this case is the cross entropy function. Therefor we create a cost function object from the cost_function_PyTorch class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c0b9516c0b3c56",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-10T12:54:25.433799400Z",
     "start_time": "2023-11-10T12:54:25.424809300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'MSELoss()'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critertion = nn.MSELoss()\n",
    "str(critertion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e93aa1992e1adf",
   "metadata": {},
   "source": [
    "Now we calculate the cost function for the output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adfcb18df669fe",
   "metadata": {},
   "source": [
    "6. Calculate the gradients needed for the back propagation part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73aba0685d0c47",
   "metadata": {},
   "source": [
    "In order to perform the back propagation part, we need to calculate the gradients of the cost function with respect to the weights and biases. This is done by the autograd package of PyTorch. We therefor initiate an optimizer object from the Gradient_Descent_PyTorch class based on the designed neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b823f2494905e0bb",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-10T12:54:26.308098100Z",
     "start_time": "2023-11-10T12:54:25.441964100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.Adam(ffnn.parameters(), learning_rate)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd3f0d2701c398",
   "metadata": {},
   "source": [
    "4. Perform the feed-forward pass and calculate the output of the network.\n",
    "\n",
    "7. Use the gradients to train the network in the back propagation part. Think of using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89e0d985c5ed46e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T12:55:52.730247300Z",
     "start_time": "2023-11-10T12:55:51.870756800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0:\n",
      " Loss: 2.210644743172452e-05 \n",
      " accuracy: 1.0\n",
      "Epoch: 100:\n",
      " Loss: 0.0020973042119294405 \n",
      " accuracy: 1.0\n",
      "Epoch: 200:\n",
      " Loss: 0.0018934904364868999 \n",
      " accuracy: 1.0\n",
      "Epoch: 300:\n",
      " Loss: 0.0017163539305329323 \n",
      " accuracy: 1.0\n",
      "Epoch: 400:\n",
      " Loss: 0.0015613837167620659 \n",
      " accuracy: 1.0\n",
      "Epoch: 500:\n",
      " Loss: 0.0014250017702579498 \n",
      " accuracy: 1.0\n",
      "Epoch: 600:\n",
      " Loss: 0.0013043283252045512 \n",
      " accuracy: 1.0\n",
      "Epoch: 700:\n",
      " Loss: 0.001197035307995975 \n",
      " accuracy: 1.0\n",
      "Epoch: 800:\n",
      " Loss: 0.0011012147879227996 \n",
      " accuracy: 1.0\n",
      "Epoch: 900:\n",
      " Loss: 0.0010152923641726375 \n",
      " accuracy: 1.0\n",
      "[0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "input = X\n",
    "all_losses = []\n",
    "current_loss = 0\n",
    "plot_every = 100\n",
    "for epoch in range(1000):\n",
    "    ffnn.train()\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()    \n",
    "\n",
    "    # Forward pass to get output/logits\n",
    "    target_pred = ffnn.feed_forward(input)\n",
    "    \n",
    "    # Calculate loss and do backpropagation\n",
    "    loss = critertion(target_pred, target_XOR)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating neural network parameters: w = w - learning_rate * gradient\n",
    "    optimizer.step()    \n",
    "    current_loss += loss\n",
    "    \n",
    "    \n",
    "    if epoch % plot_every == 0:\n",
    "        acc = accuracy_score(ffnn.classify(target_pred), target_XOR.detach().numpy())\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        \n",
    "        print(f'Epoch: {epoch}:\\n Loss: {current_loss / plot_every} \\n accuracy: {acc}')\n",
    "        current_loss = 0\n",
    "\n",
    "print(ffnn.classify(target_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
