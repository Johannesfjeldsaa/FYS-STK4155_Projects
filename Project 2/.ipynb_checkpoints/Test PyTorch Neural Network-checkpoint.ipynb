{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cad4281a763db6",
   "metadata": {},
   "source": [
    "# Neural Network with PyTorch framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efef9246badaac9",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T12:11:08.026099900Z",
     "start_time": "2023-11-07T12:11:01.022410100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from Neural_Network_with_PyTorch import Neural_Network_PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d705eb5bf84d3",
   "metadata": {},
   "source": [
    "## Create Neural network and perform passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f52acacf316be3",
   "metadata": {},
   "source": [
    "1. Set up the design matrix with the inputs as discussed above and a vector containing the output, the so-called targets. Note that the design matrix is the same for all gates. You need just to define different outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7839efb5e03f7e",
   "metadata": {},
   "source": [
    "We start by defining a helper function that allows us to convert a numpy array to a PyTorch tensor, with or without the gradient flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6ad790cc4362ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T12:11:14.465632600Z",
     "start_time": "2023-11-07T12:11:14.411227700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create design matrix\n",
    "X = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).requires_grad_(True)\n",
    "\n",
    "# The XOR gate\n",
    "target_XOR = torch.Tensor([ 0, 1 ,1, 0]).view(-1, 1)\n",
    "# The OR gate\n",
    "target_OR = torch.Tensor([ 0, 1 ,1, 1]).view(-1, 1)\n",
    "# The AND gate\n",
    "target_AND = torch.Tensor([ 0, 0 ,0, 1]).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28088293c725bdc",
   "metadata": {},
   "source": [
    "2. Construct a neural network with only one hidden layer and two hidden nodes using the Sigmoid function as activation function.\n",
    "3. Set up the output layer with only one output node and use again the Sigmoid function as activation function for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca689bc2603bc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T12:12:00.657755900Z",
     "start_time": "2023-11-07T12:12:00.623004300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters\n",
      "[Parameter containing:\n",
      "tensor([[-0.3677, -0.0116],\n",
      "        [ 0.0052,  0.1841]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5088, -0.6911], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3727, -0.1124]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5685], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# Network design\n",
    "n_inputs = X.shape[1]\n",
    "n_hidden_layers = 1\n",
    "n_hidden_nodes = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# Create network\n",
    "ffnn = Neural_Network_PyTorch(n_inputs, \n",
    "                              n_hidden_layers, n_hidden_nodes, \n",
    "                              n_outputs, \n",
    "                              activation_function_hidden_layers='sigmoid',\n",
    "                              activation_function_output_layer='sigmoid')\n",
    "\n",
    "print('Model parameters')\n",
    "print(list(ffnn.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b3d3307d5ccd",
   "metadata": {},
   "source": [
    "5. Set up the cost function (cross entropy for classification of binary cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c795cd3605117299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T12:11:57.386008100Z",
     "start_time": "2023-11-07T12:11:57.370325700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.5531, 0.4524], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3246, -0.3257],\n",
      "        [-0.6374,  0.0741]], requires_grad=True)\n",
      "Sigmoid()\n"
     ]
    }
   ],
   "source": [
    "for layer in ffnn.hidden_layers:\n",
    "    print(layer[0].bias)\n",
    "    print(layer[0].weight)\n",
    "    print(layer[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18afac54d5b5fc6",
   "metadata": {},
   "source": [
    "The evaluation criterion in this case is the cross entropy function. Therefor we create a cost function object from the cost_function_PyTorch class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c0b9516c0b3c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:31:49.911856900Z",
     "start_time": "2023-11-05T19:31:49.884782900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CrossEntropyLoss()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critertion = nn.CrossEntropyLoss()\n",
    "str(critertion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e93aa1992e1adf",
   "metadata": {},
   "source": [
    "Now we calculate the cost function for the output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adfcb18df669fe",
   "metadata": {},
   "source": [
    "6. Calculate the gradients needed for the back propagation part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73aba0685d0c47",
   "metadata": {},
   "source": [
    "In order to perform the back propagation part, we need to calculate the gradients of the cost function with respect to the weights and biases. This is done by the autograd package of PyTorch. We therefor initiate an optimizer object from the Gradient_Descent_PyTorch class based on the designed neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b823f2494905e0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:31:53.124797800Z",
     "start_time": "2023-11-05T19:31:52.645253500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(ffnn\u001b[38;5;241m.\u001b[39mparameters(), learning_rate)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimizer)\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\netcdf_DataAnalysis\\Lib\\site-packages\\torch\\optim\\adam.py:33\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(weight_decay))\n\u001b[0;32m     29\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     30\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     31\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     32\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\netcdf_DataAnalysis\\Lib\\site-packages\\torch\\optim\\optimizer.py:187\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    185\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    189\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.Adam(ffnn.parameters(), learning_rate)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd3f0d2701c398",
   "metadata": {},
   "source": [
    "4. Perform the feed-forward pass and calculate the output of the network.\n",
    "\n",
    "7. Use the gradients to train the network in the back propagation part. Think of using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e0d985c5ed46e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:33:51.575424300Z",
     "start_time": "2023-11-05T19:33:27.138442600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input = X\n",
    "all_losses = []\n",
    "current_loss = 0\n",
    "plot_every = 100\n",
    "for epoch in tqdm(range(100000), desc=\"Training\"):\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()    \n",
    "\n",
    "    # Forward pass to get output/logits\n",
    "    target_pred = ffnn.feed_forward(input)\n",
    "    \n",
    "    # Calculate loss and do backpropagation\n",
    "    loss = critertion(target_pred, target_XOR)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating neural network parameters: w = w - learning_rate * gradient\n",
    "    optimizer.step()    \n",
    "    current_loss += loss\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        print('Epoch: {} and Loss: {}'.format(epoch, loss))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656b0ed2e147ab0",
   "metadata": {},
   "source": [
    "P"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
